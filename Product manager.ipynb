{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71a6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18400c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from google import genai\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any, Dict, Optional\n",
    "from memory_system import (\n",
    "    MemoryStore, MemoryRetriever, FeedbackHandler, \n",
    "    ContextBuilder, AgentMemoryIntegration\n",
    ")\n",
    "from reflection_system import ReflectionWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc620c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = genai.Client(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "model=\"gemini-2.5-pro\"\n",
    "tavily_client = TavilyClient(api_key=os.environ.get('TAVILY_API_KEY')) # Initialize Tavily client\n",
    "\n",
    "# Memory\n",
    "\n",
    "# Initialize memory system\n",
    "memory_store = MemoryStore(\"pm_agent_memory.db\")\n",
    "retriever = MemoryRetriever(memory_store)\n",
    "feedback_handler = FeedbackHandler(memory_store)\n",
    "context_builder = ContextBuilder(retriever)\n",
    "memory_integration = AgentMemoryIntegration(\n",
    "    memory_store, retriever, feedback_handler, context_builder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a37c11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agents\n",
    "\n",
    "class StatisticianAgent:\n",
    "    \"\"\"Statistician Agent that provides statistical validation, methodology suggestions, and risk analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration, num_reflections: int = 0):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow = ReflectionWorkflow(client, model, num_reflections)\n",
    "        self.system_instruction = \"\"\"You are an expert Statistician and Data Scientist with deep expertise in:\n",
    "        - Experimental design and A/B testing methodologies\n",
    "        - Causal inference techniques (propensity score matching, instrumental variables, difference-in-differences, RCTs)\n",
    "        - Statistical hypothesis testing and power analysis\n",
    "        - Observational study design and confounding control\n",
    "        - Sample size calculations and statistical power\n",
    "        - Bayesian and frequentist inference\n",
    "        - Time series analysis and longitudinal data methods\n",
    "        - Machine learning validation and model evaluation\n",
    "        - Data quality assessment and bias detection\n",
    "        - Privacy-preserving statistical methods\n",
    "        - Communicating statistical concepts to non-technical stakeholders\n",
    "\n",
    "        You analyze product plans, research designs, and data strategies to:\n",
    "        1. Validate statistical assumptions and methodological soundness\n",
    "        2. Suggest appropriate statistical techniques and experimental designs\n",
    "        3. Flag potential risks, biases, and confounding factors\n",
    "        4. Provide specific, actionable recommendations with clear rationale\n",
    "        5. Assess feasibility of causal claims and required data infrastructure\n",
    "        \n",
    "        You communicate complex statistical concepts clearly, always considering practical constraints\n",
    "        like sample size, data collection feasibility, and computational resources.\"\"\"\n",
    "    \n",
    "    def __call__(self, analysis_request: str, num_reflections: Optional[int] = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Analyze plans/designs from a statistical perspective.\n",
    "        \n",
    "        Args:\n",
    "            analysis_request: The plan, design, or question to analyze (from user, PM, or UX researcher)\n",
    "            num_reflections: Number of reflections for this specific query\n",
    "            verbose: If True, returns reflection data\n",
    "            \n",
    "        Returns:\n",
    "            (response, memory_id) tuple or (response, memory_id, reflection_data) if verbose\n",
    "        \"\"\"\n",
    "        reflections_to_use = num_reflections if num_reflections is not None else self.num_reflections\n",
    "        \n",
    "        # Get memory context\n",
    "        memory_context = self.memory_integration.pre_run_hook(analysis_request, \"statistical_analysis\")\n",
    "\n",
    "        # Check if web search needed for recent statistical methods or research\n",
    "        decision_prompt = f\"\"\"Analyze this statistical analysis request and determine if you need to search for \n",
    "recent statistical methodologies, causal inference techniques, or research papers.\n",
    "\n",
    "Request: {analysis_request}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\":\"\"\"\n",
    "\n",
    "        decision_response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=decision_prompt\n",
    "        )\n",
    "        \n",
    "        needs_search = \"YES\" in decision_response.text.strip().upper()\n",
    "        \n",
    "        search_context = \"\"\n",
    "        if needs_search:\n",
    "            print(\"ðŸ” Searching for statistical methods and research...\")\n",
    "            search_results = search_web(f\"statistical methodology causal inference {analysis_request[:100]}\")\n",
    "            if isinstance(search_results, list):\n",
    "                search_context = \"\\n\\nRecent Statistical Methods & Research:\\n\"\n",
    "                for i, result in enumerate(search_results, 1):\n",
    "                    search_context += f\"\\n{i}. {result.get('title', '')}\\n{result.get('content', '')}\\n\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious statistical analyses:\\n\"\n",
    "            for entry in self.conversation_history[-3:]:\n",
    "                conversation_context += f\"Request: {entry['request']}\\nAnalysis: {entry['response'][:300]}...\\n\\n\"\n",
    "\n",
    "        def generate_response(query: str) -> str:\n",
    "            full_query = (\n",
    "                memory_context +\n",
    "                conversation_context + \n",
    "                f\"\\nProvide statistical analysis for:\\n{query}\\n\\n\"\n",
    "                \"Structure your response with:\\n\"\n",
    "                \"1. **Validation of Assumptions**: What statistical assumptions are being made?\\n\"\n",
    "                \"2. **Recommended Techniques**: Which specific statistical methods should be used?\\n\"\n",
    "                \"3. **Risk Warnings**: What are the key statistical risks and limitations?\\n\"\n",
    "                \"4. **Implementation Details**: Sample sizes, power calculations, data requirements\\n\"\n",
    "                \"5. **Causal Inference Considerations**: If applicable, how to establish causality\" +\n",
    "                search_context\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_query,\n",
    "                config={'system_instruction': self.system_instruction}\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        if reflections_to_use > 0:\n",
    "            print(f\"ðŸ¤” Reflecting {reflections_to_use} time(s)...\")\n",
    "            self.reflection_workflow.num_reflections = reflections_to_use\n",
    "            result = self.reflection_workflow.execute(\n",
    "                initial_query=analysis_request,\n",
    "                generate_response_fn=generate_response,\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "            final_response = result['final_response']\n",
    "            reflection_data = result\n",
    "        else:\n",
    "            final_response = generate_response(analysis_request)\n",
    "            reflection_data = None\n",
    "\n",
    "        self.conversation_history.append({\n",
    "            'request': analysis_request,\n",
    "            'response': final_response\n",
    "        })\n",
    "        \n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            analysis_request,\n",
    "            final_response,\n",
    "            memory_type=\"statistical_analysis\",\n",
    "            metadata={\n",
    "                'search_used': needs_search,\n",
    "                'num_reflections': reflections_to_use\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if verbose and reflection_data:\n",
    "            return final_response, memory_id, reflection_data\n",
    "        \n",
    "        return final_response, memory_id\n",
    "    \n",
    "    def set_reflections(self, num_reflections: int):\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow.num_reflections = num_reflections\n",
    "        print(f\"Default reflection count updated to: {num_reflections}\")\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, feedback_content: str = \"\"):\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "        self.reflection_workflow.clear_history()\n",
    "        print(\"Statistical analysis history cleared.\")\n",
    "\n",
    "class UXResearcherAgent:\n",
    "    \"\"\"UX Researcher Agent that conducts user research based on product briefs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration, num_reflections: int = 0):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow = ReflectionWorkflow(client, model, num_reflections)\n",
    "        self.system_instruction = \"\"\"You are an experienced UX Researcher with expertise in:\n",
    "        - User interviews and surveys\n",
    "        - Usability testing and evaluation\n",
    "        - User personas and journey mapping\n",
    "        - Information architecture\n",
    "        - Behavioral analysis and user psychology\n",
    "        - A/B testing and experimentation\n",
    "        - Accessibility and inclusive design\n",
    "        - Analytics interpretation\n",
    "        - Presenting research findings and recommendations\n",
    "\n",
    "        You analyze product briefs and provide detailed UX research plans, methodologies, \n",
    "        and insights. You ask clarifying questions about target users, use cases, and \n",
    "        constraints. You base your recommendations on established UX principles and research methods.\"\"\"\n",
    "    \n",
    "    def __call__(self, brief: str, num_reflections: Optional[int] = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Process a product brief and provide UX research recommendations.\n",
    "        \n",
    "        Args:\n",
    "            brief: The product brief (from user or PM agent)\n",
    "            num_reflections: Number of reflections for this specific query\n",
    "            verbose: If True, returns reflection data\n",
    "            \n",
    "        Returns:\n",
    "            (response, memory_id) tuple or (response, memory_id, reflection_data) if verbose\n",
    "        \"\"\"\n",
    "        reflections_to_use = num_reflections if num_reflections is not None else self.num_reflections\n",
    "        \n",
    "        # Get memory context\n",
    "        memory_context = self.memory_integration.pre_run_hook(brief, \"ux_research\")\n",
    "\n",
    "        # Check if web search needed for UX best practices\n",
    "        decision_prompt = f\"\"\"Analyze this UX research brief and determine if you need to search for current UX best practices, research methodologies, or industry standards.\n",
    "\n",
    "Brief: {brief}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\":\"\"\"\n",
    "\n",
    "        decision_response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=decision_prompt\n",
    "        )\n",
    "        \n",
    "        needs_search = \"YES\" in decision_response.text.strip().upper()\n",
    "        \n",
    "        search_context = \"\"\n",
    "        if needs_search:\n",
    "            print(\"ðŸ” Searching for UX research best practices...\")\n",
    "            search_results = search_web(f\"UX research methodology {brief[:100]}\")\n",
    "            if isinstance(search_results, list):\n",
    "                search_context = \"\\n\\nUX Research Best Practices:\\n\"\n",
    "                for i, result in enumerate(search_results, 1):\n",
    "                    search_context += f\"\\n{i}. {result.get('title', '')}\\n{result.get('content', '')}\\n\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious research context:\\n\"\n",
    "            for entry in self.conversation_history[-3:]:\n",
    "                conversation_context += f\"Brief: {entry['brief']}\\nResearch Plan: {entry['response']}\\n\\n\"\n",
    "\n",
    "        def generate_response(query: str) -> str:\n",
    "            full_query = (\n",
    "                memory_context +\n",
    "                conversation_context + \n",
    "                f\"\\n{query}\" +\n",
    "                search_context\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_query,\n",
    "                config={'system_instruction': self.system_instruction}\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        if reflections_to_use > 0:\n",
    "            print(f\"ðŸ¤” Reflecting {reflections_to_use} time(s)...\")\n",
    "            self.reflection_workflow.num_reflections = reflections_to_use\n",
    "            result = self.reflection_workflow.execute(\n",
    "                initial_query=brief,\n",
    "                generate_response_fn=generate_response,\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "            final_response = result['final_response']\n",
    "            reflection_data = result\n",
    "        else:\n",
    "            final_response = generate_response(brief)\n",
    "            reflection_data = None\n",
    "\n",
    "        self.conversation_history.append({\n",
    "            'brief': brief,\n",
    "            'response': final_response\n",
    "        })\n",
    "        \n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            brief,\n",
    "            final_response,\n",
    "            memory_type=\"ux_research\",\n",
    "            metadata={\n",
    "                'search_used': needs_search,\n",
    "                'num_reflections': reflections_to_use\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if verbose and reflection_data:\n",
    "            return final_response, memory_id, reflection_data\n",
    "        \n",
    "        return final_response, memory_id\n",
    "    \n",
    "    def set_reflections(self, num_reflections: int):\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow.num_reflections = num_reflections\n",
    "        print(f\"Default reflection count updated to: {num_reflections}\")\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, feedback_content: str = \"\"):\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "        self.reflection_workflow.clear_history()\n",
    "        print(\"Research history cleared.\")\n",
    "\n",
    "class SampleDataGeneratorAgent:\n",
    "    \"\"\"Agent that generates sample/mock data for testing and prototyping.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration, num_reflections: int = 0):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow = ReflectionWorkflow(client, model, num_reflections)\n",
    "        self.system_instruction = \"\"\"You are a Sample Data Generator specialist with expertise in:\n",
    "        - Creating realistic test data and mock datasets\n",
    "        - Understanding data schemas and structures\n",
    "        - Generating diverse, representative data samples\n",
    "        - Ensuring data privacy and anonymization\n",
    "        - Creating edge cases and boundary conditions\n",
    "        - Generating data in various formats (JSON, CSV, SQL, etc.)\n",
    "        - Synthetic data generation techniques\n",
    "        - Data volume scaling considerations\n",
    "\n",
    "        You generate realistic, contextually appropriate sample data based on specifications.\n",
    "        You ensure data quality, diversity, and adherence to constraints. You can generate\n",
    "        data for user profiles, transactions, interactions, or any other entities needed\n",
    "        for testing and prototyping.\"\"\"\n",
    "    \n",
    "    def __call__(self, specification: str, num_reflections: Optional[int] = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Generate sample data based on specification.\n",
    "        \n",
    "        Args:\n",
    "            specification: Description of data to generate (schema, format, volume, etc.)\n",
    "            num_reflections: Number of reflections for this specific query\n",
    "            verbose: If True, returns reflection data\n",
    "            \n",
    "        Returns:\n",
    "            (response, memory_id) tuple or (response, memory_id, reflection_data) if verbose\n",
    "        \"\"\"\n",
    "        reflections_to_use = num_reflections if num_reflections is not None else self.num_reflections\n",
    "        \n",
    "        memory_context = self.memory_integration.pre_run_hook(specification, \"data_generation\")\n",
    "\n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious data generation context:\\n\"\n",
    "            for entry in self.conversation_history[-3:]:\n",
    "                conversation_context += f\"Spec: {entry['spec']}\\n\\n\"\n",
    "\n",
    "        def generate_response(query: str) -> str:\n",
    "            full_query = (\n",
    "                memory_context +\n",
    "                conversation_context + \n",
    "                f\"\\nGenerate sample data based on: {query}\\n\\n\"\n",
    "                \"Provide the data in a structured format with clear labels and explanations.\"\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_query,\n",
    "                config={'system_instruction': self.system_instruction}\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        if reflections_to_use > 0:\n",
    "            print(f\"ðŸ¤” Reflecting {reflections_to_use} time(s)...\")\n",
    "            self.reflection_workflow.num_reflections = reflections_to_use\n",
    "            result = self.reflection_workflow.execute(\n",
    "                initial_query=specification,\n",
    "                generate_response_fn=generate_response,\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "            final_response = result['final_response']\n",
    "            reflection_data = result\n",
    "        else:\n",
    "            final_response = generate_response(specification)\n",
    "            reflection_data = None\n",
    "\n",
    "        self.conversation_history.append({\n",
    "            'spec': specification,\n",
    "            'response': final_response\n",
    "        })\n",
    "        \n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            specification,\n",
    "            final_response,\n",
    "            memory_type=\"data_generation\",\n",
    "            metadata={'num_reflections': reflections_to_use}\n",
    "        )\n",
    "\n",
    "        if verbose and reflection_data:\n",
    "            return final_response, memory_id, reflection_data\n",
    "        \n",
    "        return final_response, memory_id\n",
    "    \n",
    "    def set_reflections(self, num_reflections: int):\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow.num_reflections = num_reflections\n",
    "        print(f\"Default reflection count updated to: {num_reflections}\")\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, feedback_content: str = \"\"):\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        self.conversation_history = []\n",
    "        self.reflection_workflow.clear_history()\n",
    "        print(\"Data generation history cleared.\")\n",
    "\n",
    "class ProductManagerAgent:\n",
    "    \"\"\"Product Manager Agent with conversation memory and long-term learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration, num_reflections: int = 0):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow = ReflectionWorkflow(client, model, num_reflections)\n",
    "        self.system_instruction = \"\"\"You are an experienced Product Manager with expertise in:\n",
    "        - Product strategy and roadmap planning\n",
    "        - Market research and competitive analysis\n",
    "        - User research and requirements gathering\n",
    "        - Feature prioritization and backlog management\n",
    "        - Stakeholder communication\n",
    "        - Metrics and KPI definition\n",
    "        - Go-to-market strategies\n",
    "        - Agile methodologies\n",
    "        - Providing feedback to your product specialist, engineering, design, data science, testing, and marketing teams\n",
    "\n",
    "        You provide practical, actionable advice and ask clarifying questions when needed.\n",
    "        When provided with search results or past memories, incorporate relevant information into your response and cite sources.\"\"\"\n",
    "    \n",
    "    def __call__(self, user_query: str, num_reflections: Optional[int] = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Make the agent callable like a function.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's query\n",
    "            num_reflections: Number of reflections for this specific query (overrides default)\n",
    "            verbose: If True, returns reflection data\n",
    "            \n",
    "        Returns:\n",
    "            (response, memory_id) tuple or (response, memory_id, reflection_data) if verbose\n",
    "        \"\"\"\n",
    "        # Use query-specific reflections or fall back to instance default\n",
    "        reflections_to_use = num_reflections if num_reflections is not None else self.num_reflections\n",
    "        \n",
    "        # Get memory context\n",
    "        memory_context = self.memory_integration.pre_run_hook(user_query, \"pm_interaction\")\n",
    "\n",
    "        # Decide if search is needed\n",
    "        decision_prompt = f\"\"\"Analyze this query and determine if you need web search.\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\":\"\"\"\n",
    "\n",
    "        decision_response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=decision_prompt\n",
    "        )\n",
    "        \n",
    "        needs_search = \"YES\" in decision_response.text.strip().upper()\n",
    "        \n",
    "        # Perform search if needed\n",
    "        search_context = \"\"\n",
    "        if needs_search:\n",
    "            print(\"ðŸ” Searching the web...\")\n",
    "            search_results = search_web(user_query)\n",
    "            if isinstance(search_results, list):\n",
    "                search_context = \"\\n\\nWeb Search Results:\\n\"\n",
    "                for i, result in enumerate(search_results, 1):\n",
    "                    search_context += f\"\\n{i}. {result.get('title', '')}\\n{result.get('content', '')}\\n\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "            for entry in self.conversation_history[-5:]:  # Keep last 5 exchanges\n",
    "                conversation_context += f\"User: {entry['user']}\\nAssistant: {entry['assistant']}\\n\\n\"\n",
    "\n",
    "        # Define response generation function for reflection workflow\n",
    "        def generate_response(query: str) -> str:\n",
    "            full_query = (\n",
    "                memory_context +\n",
    "                conversation_context + \n",
    "                f\"\\n{query}\" +\n",
    "                search_context\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_query,\n",
    "                config={'system_instruction': self.system_instruction}\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        # Execute with or without reflection\n",
    "        if reflections_to_use > 0:\n",
    "            print(f\"ðŸ¤” Reflecting {reflections_to_use} time(s)...\")\n",
    "            # Temporarily update reflection workflow\n",
    "            self.reflection_workflow.num_reflections = reflections_to_use\n",
    "            result = self.reflection_workflow.execute(\n",
    "                initial_query=user_query,\n",
    "                generate_response_fn=generate_response,\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "            final_response = result['final_response']\n",
    "            reflection_data = result\n",
    "        else:\n",
    "            final_response = generate_response(user_query)\n",
    "            reflection_data = None\n",
    "\n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            'user': user_query,\n",
    "            'assistant': final_response\n",
    "        })\n",
    "        \n",
    "        # Log to long-term memory\n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            user_query,\n",
    "            final_response,\n",
    "            memory_type=\"pm_interaction\",\n",
    "            metadata={\n",
    "                'search_used': needs_search, \n",
    "                'num_reflections': reflections_to_use, \n",
    "                'reflection_used': reflections_to_use > 0\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if verbose and reflection_data:\n",
    "            return final_response, memory_id, reflection_data\n",
    "        \n",
    "        return final_response, memory_id\n",
    "    \n",
    "    def set_reflections(self, num_reflections: int):\n",
    "        \"\"\"Update the default number of reflections.\"\"\"\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow.num_reflections = num_reflections\n",
    "        print(f\"Default reflection count updated to: {num_reflections}\")\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, \n",
    "                      feedback_content: str = \"\"):\n",
    "        \"\"\"Apply feedback to a specific interaction.\"\"\"\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.reflection_workflow.clear_history()\n",
    "        print(\"Conversation history cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93a3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def remove_markdown(response_text):\n",
    "    \"\"\"Remove markdown formatting and print clean text.\"\"\"\n",
    "    # Remove markdown formatting\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', response_text)  # Bold\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'`(.+?)`', r'\\1', text)  # Code\n",
    "    text = re.sub(r'#+\\s', '', text)  # Headers\n",
    "    return text\n",
    "def chat_with_agent(agent, agent_name=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Generic interactive chat interface for any agent.\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent instance (callable) to chat with\n",
    "        agent_name: Display name for the agent (default: \"Agent\")\n",
    "    \n",
    "    Type 'exit', 'quit', or 'bye' to end the conversation.\n",
    "    Type 'clear' to clear conversation history.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{agent_name} Chat\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Start chatting with the {agent_name}!\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the chat.\")\n",
    "    print(\"Type 'clear' to clear conversation history.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(f\"\\n{agent_name}: Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'clear':\n",
    "            if hasattr(agent, 'clear_history'):\n",
    "                agent.clear_history()\n",
    "            continue\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = agent(user_input)\n",
    "            print(f\"\\n{agent_name}: {remove_markdown(response)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\\n\")\n",
    "            print(\"Please try again or type 'exit' to quit.\\n\")\n",
    "def search_web(query):\n",
    "    \"\"\"Search the web using Tavily.\"\"\"\n",
    "    try:\n",
    "        response = tavily_client.search(query, max_results=5)\n",
    "        return response['results']\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent instances\n",
    "pm_agent = ProductManagerAgent(model=model, client=client, memory_integration=memory_integration)\n",
    "ux_researcher = UXResearcherAgent(model=model, client=client, memory_integration=memory_integration)\n",
    "data_generator = SampleDataGeneratorAgent(model=model, client=client, memory_integration=memory_integration)\n",
    "statistician = StatisticianAgent(model=model, client=client, memory_integration=memory_integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b69a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"If you wanted to build an app that lets users track their daily routine and interventions, and use counterfactuals based on other users and counterfactuals using quality research to provide them with causal impact of their interventions (for e.g., you drink black coffee every morning leads to higher energy levels) or also suggests changes to their life based on causal analysis, how would you go about doing it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching the web...\n",
      "ðŸ¤” Reflecting 2 time(s)...\n",
      "Excellent. This is the right way to build a product: start with a strong strategic plan, critically evaluate it, and then refine it into a more robust, battle-tested execution plan. Your previous response and my self-critique have given us the necessary ingredients.\n",
      "\n",
      "Let's integrate that feedback. The result is not just a plan to build V1, but a comprehensive strategy to launch a platform.\n",
      "\n",
      "We have our four strategic pillars ([pm_interaction] #1, #2): Causal Data Asset, Trust & Ethics, Incremental Insight Engine, and Go-to-Market. The entire purpose of V1 is to build the first, most critical piece of the Causal Data Asset. Without a rich, high-quality, longitudinal dataset, our long-term vision of a causal engine is impossible.\n",
      "\n",
      "This execution plan is ruthlessly focused on solving that data acquisition problem while laying the foundation for everything that comes after.\n",
      "\n",
      "---\n",
      "\n",
      "Phase 1: Creating the V1 Product Requirements Document (PRD)\n",
      "\n",
      "This PRD is our single source of truth, now updated with more nuance and strategic foresight.\n",
      "\n",
      "V1 PRD: \"The Smart Diary\"\n",
      "\n",
      "1.  Problem Statement: Individuals managing chronic, fluctuating conditions like migraines lack a simple, unified tool to identify potential triggers. Existing solutions are either too generic (notes apps) or too high-friction (complex trackers). While dedicated trackers like Bearable exist, they are not architected from the ground up to create the structured, metadata-rich \"causal-ready\" data asset required for future, large-scale analysis. Our V1 differentiator is not a specific feature, but this foundational data architecture.\n",
      "\n",
      "2.  Target User (from GTM Strategy): Our initial user is the \"motivated investigator\"â€”someone actively managing migraines who is willing to track data in search of triggers. They are likely active in online communities like r/migraine.\n",
      "\n",
      "3.  Core Hypothesis: We believe that by providing a sub-30-second daily logging flow for this highly-motivated group, we can achieve >60% daily tracking adherence over a 30-day period, validating that the core behavior of our entire platform is viable.\n",
      "\n",
      "4.  V1 Success Metrics (KPIs):\n",
      "    *   Primary (Engagement): Tracking Adherence > 60% over the first 30 days.\n",
      "    *   Secondary (Value): Net Promoter Score (NPS) > 40 from beta users, indicating we've solved a real pain point.\n",
      "    *   Leading (Trust): Anonymized Data Sharing Opt-in Rate > 25%. This is a powerful leading indicator of trust in our mission.\n",
      "\n",
      "5.  V1 Non-Goals (Disciplined Exclusion):\n",
      "    *   We will not show any causal or population-level insights.\n",
      "    *   We will not build any social or community features.\n",
      "    *   We will not suggest any interventions or experiments.\n",
      "\n",
      "6.  Non-Functional & Foundational Requirements:\n",
      "       Security & Privacy (HIPAA-Inspired): The system must be built with security and privacy best practices inspired by* HIPAA standards (e.g., encryption at rest and in transit). This ensures we are on a path to future compliance without overcommitting in V1.\n",
      "    *   Causal-Ready Data Schema: The backend must use the unified, metadata-rich schema to support future analysis. This is a non-negotiable architectural decision.\n",
      "    *   Trust & Transparency by Design: V1 must include a \"Plain English\" data usage policy, separate from the legal ToS.\n",
      "    *   Responsible UX for Correlations: The design of any personal data visualizations must actively mitigate the risk of users confusing correlation with causation, using clear language and educational components.\n",
      "\n",
      "---\n",
      "\n",
      "Phase 2: The Cross-Functional V1 Kick-Off\n",
      "\n",
      "With this more robust PRD, my communication to the team becomes more targeted.\n",
      "\n",
      "*   To the Engineering Lead: \"Our two biggest challenges are foundational: a HIPAA-inspired security architecture and the 'causal-ready' data schema. Getting the schema right from day one is our primary technical goal, as it's the foundation of our entire company vision. The analytics instrumentation to measure adherence is just as critical.\"\n",
      "\n",
      "*   To the Design/UX Lead: \"Your team faces two core challenges. First, nailing the sub-30-second logging flow is paramount to our core hypothesis. Second, and just as important, is a critical UX ethics challenge: How do we design the personal correlation visualizer to provide value while actively educating the user on the difference between correlation and causation? We must prevent users from drawing premature, incorrect conclusions. This is a direct execution of our 'Trust & Ethics' pillar.\"\n",
      "\n",
      "*   To the Data Science Lead: \"Your mission in V1 is to architect our Causal Data Asset by partnering with engineering on the schema. As soon as data flows from our first users, you will run exploratory analysis to find data quality issues. Crucially, I need you to define the data density and user volume thresholds that will signal we are ready to begin developing the V2 correlational insight models.\"\n",
      "\n",
      "*   To the Marketing Lead: \"Your mission is surgical: find our first 1,000 beta users from the migraine community. We need a cohort of highly motivated 'motivated investigators' who will provide the rich feedback and consistent data required to validate our hypothesis.\"\n",
      "\n",
      "---\n",
      "\n",
      "Phase 3: The Path to Causality (Our Data-Driven Roadmap)\n",
      "\n",
      "This section explicitly connects our pragmatic V1 to the ambitious vision, showing the team and stakeholders our deliberate, staged path forward. This operationalizes our \"Incremental Insight Engine\" pillar ([pm_interaction] #1, #4).\n",
      "\n",
      "*   V1: The \"Smart Diary\" (Current Phase)\n",
      "    *   Goal: Achieve data liquidity. Can we build a tool people use consistently to generate our core data asset?\n",
      "    *   Exit Criteria: >60% tracking adherence and >40 NPS from our beta cohort.\n",
      "\n",
      "*   V2: The \"Pattern Spotter\"\n",
      "    *   Goal: Introduce sophisticated, privacy-preserving correlational insights.\n",
      "    *   Trigger: We begin development once we have >1,000 daily active users and our data asset has sufficient density.\n",
      "    *   Features: Begin surfacing carefully framed population-level insights: \"Users like you who log X also tend to report Y.\" Introduce the N-of-1 Coach to guide personal experiments.\n",
      "\n",
      "*   V3: The \"Causal Inference Engine\"\n",
      "    *   Goal: Begin delivering true causal insights on observational data.\n",
      "    *   Trigger: We begin R&D once our dataset is massive and longitudinal (e.g., millions of user-days of data).\n",
      "    *   Features: Use advanced statistical methods (e.g., propensity score matching) to create \"digital twins\" and estimate the causal impact of specific interventions, always framed with confidence intervals and disclaimers.\n",
      "\n",
      "*   V4: The \"Evidence-Synthesizing Engine\"\n",
      "    *   Goal: Become a trusted authority by merging user data with scientific literature.\n",
      "    *   Features: Integrate our causal findings with a knowledge graph of peer-reviewed studies to provide insights like: \"Our data suggests X may impact Y for people like you. This is supported by [this clinical study].\"\n",
      "\n",
      "This operational plan ensures we build V1 with discipline while never losing sight of our ambitious vision. We are methodically building the foundation of data, trust, and user behavior required to one day deliver a world-class causal inference engine.\n",
      "\n",
      "Given this refined plan, what do you now see as the single most difficult challenge for the V1 team: the technical challenge of building the \"causal-ready\" data schema correctly, or the design challenge of creating a user experience that responsibly displays personal correlations without misleading users?\n"
     ]
    }
   ],
   "source": [
    "response, memory_id = pm_agent(query, num_reflections=2)\n",
    "print(remove_markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59cf222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for UX research best practices...\n",
      "ðŸ¤” Reflecting 2 time(s)...\n",
      "Excellent. This refined operational plan is a masterclass in translating a long-term vision into a disciplined, focused V1. The clarity of the PRD, particularly the non-goals and foundational requirements, gives the team the precise guardrails needed for execution. The role-specific kick-off points are perfect for creating alignment and a shared sense of mission.\n",
      "\n",
      "As the UX Researcher, my role is to create a research strategy that de-risks our core assumptions and ensures we are building a truly user-centric product. This plan allows me to do just that.\n",
      "\n",
      "Now, to your new, more nuanced question:\n",
      "\n",
      "Given this refined plan, what do you now see as the single most difficult challenge for the V1 team: the technical challenge of building the \"causal-ready\" data schema correctly, or the design challenge of creating a user experience that responsibly displays personal correlations without misleading users?\n",
      "\n",
      "This is the central strategic tension of V1, pitting our \"Causal Data Asset\" pillar against our \"Trust & Ethics\" pillar. Before answering, I'll restate my assumption that for V1, \"displaying personal correlations\" refers to simple visualizations of a user's own tracked data (e.g., a chart of their sleep vs. migraine severity), not population-level insights.\n",
      "\n",
      "While both are formidable challenges, one sits at the volatile intersection of human psychology and product value. Therefore, it's critical to view these risks in a hierarchy:\n",
      "\n",
      "1.  The Foundational Behavioral Challenge: As identified previously, our primary hurdle is overcoming tracking fatigue to hit our >60% adherence KPI. If we fail here, neither of the other challenges matters.\n",
      "2.  The User-Facing Design Challenge: Of the two you asked about, this is the more difficult and higher-stakes problem.\n",
      "3.  The Foundational Technical Challenge: While incredibly complex, this is a more containable problem.\n",
      "\n",
      "So, while the adherence problem is our biggest overall risk, the design challenge of responsibly displaying correlations is the more difficult of the two you've presented. Here is my reasoning.\n",
      "\n",
      "---\n",
      "\n",
      "Why the Design Challenge is Harder and More Dangerous\n",
      "\n",
      "The Technical Challenge: A Problem of Foresight and Precision\n",
      "Building the \"causal-ready\" schema is a profoundly complex engineering and data science task. Calling it purely \"deterministic\" would understate its ambiguity; it's a predictive design problem that requires immense foresight to model data for future, as-yet-undefined analyses. A mistake here creates foundational technical debt.\n",
      "\n",
      "However, it is a challenge that can be solved within our team through expertise, discipline, and established best practices in data modeling and secure architecture. The variables are largely internal.\n",
      "\n",
      "The Design Challenge: A Problem of Human Psychology and Ambiguity\n",
      "This is fundamentally harder because we are engineering for the complexities, biases, and vulnerabilities of the human mind. The variables are external and unpredictable.\n",
      "\n",
      "1.  Fighting Innate Human Cognition: We are battling apopheniaâ€”the brain's hardwired tendency to see meaningful patterns in random data. A simple disclaimer is like a small \"Caution\" sign next to a cliff edge. The entire designâ€”the chart type, colors, copy, and visual hierarchyâ€”must be architected to actively guide users toward cautious interpretation, which runs counter to their natural instincts.\n",
      "\n",
      "2.  The Razor's Edge of Value vs. Harm: To hit our NPS > 40 KPI, the app must provide value. This means the visualizations must be compelling enough to reveal potential patterns. Yet, the more compelling the pattern, the higher the risk a user will misinterpret it as causal and take potentially harmful action. This is a profound ethical and design challenge that directly impacts user safety and trust.\n",
      "\n",
      "3.  The Asymmetry of Trust: Trust is our bedrock. It is built slowly and shattered instantly. A single misleading chart that causes a user to draw a false conclusion could permanently destroy their trust in our product and our mission. This directly threatens our leading \"Trust KPI\" (Anonymized Data Opt-in Rate) and the long-term health of our data asset.\n",
      "\n",
      "---\n",
      "\n",
      "A Unified UX Research Strategy to De-Risk Both Challenges\n",
      "\n",
      "Crucially, these two challenges are not independent. A poorly designed UI pollutes the data asset, and a poorly designed schema makes a responsible UI impossible. Therefore, our UX research strategy must address both in parallel.\n",
      "\n",
      "1. De-risking the Design Challenge (Mitigating Harm & Ensuring Value)\n",
      "\n",
      "My plan remains focused on understanding and guiding user interpretation:\n",
      "*   Comprehension Testing: We will rigorously test prototypes of visualizations not by asking \"Can you use this?\" but by asking \"What is this chart telling you?\" and \"Based only on this, what action might you take?\" The answers will directly inform the UI and educational components needed to prevent misinterpretation.\n",
      "*   A/B Testing on Framing: We will test variations of language, disclaimers, and visual cues (e.g., dotted vs. solid lines) to find the combination that most effectively communicates uncertainty and enhances the user experience without causing confusion.\n",
      "*   Longitudinal Diary Study Follow-up: This long-term research method will be key. During the beta, I will interview users about the patterns they see: \"Tell me about something you noticed. What did you think? Did it change your behavior?\" This provides invaluable real-world insight into whether our design is working as intended.\n",
      "\n",
      "2. De-risking the Technical Challenge (Informing the Schema)\n",
      "\n",
      "This is a critical addition. UX research is not just for UI; it's for ensuring the foundational architecture of the product is built around a deep understanding of the user. We will directly support the engineering and data science leads by:\n",
      "*   Conducting Generative Interviews: We will go deep into the user's mental model. How do they talk about their migraines? What language do they use for symptoms, triggers, treatments, and lifestyle factors?\n",
      "*   Running Card Sorting & Tree Testing Exercises: We'll ask users to group and categorize dozens of potential factors. This data-driven approach helps us understand their inherent information architecture.\n",
      "*   Developing a User-Centered Ontology: The output of this research will be a foundational map of our users' world. This ensures our \"causal-ready\" schema is built around how users actually think and behave, not just our internal technical assumptions. It makes the schema more robust, intuitive, and truly ready for future analysis.\n",
      "\n",
      "In conclusion, while the schema is the foundation, the design of the correlation display is the first, most delicate room we are inviting users into. The technical challenge is like engineering the skyscraper's foundationâ€”a critical task of precision and foresight. The design challenge is like designing the building's emergency evacuation systemâ€”a harder problem because it depends entirely on the unpredictable, high-stakes variable of human psychology under pressure. Our research strategy is designed to provide the blueprints for both.\n"
     ]
    }
   ],
   "source": [
    "ux_response, ux_memory_id = ux_researcher(response, num_reflections=2)\n",
    "print(remove_markdown(ux_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7333509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for statistical methods and research...\n",
      "ðŸ¤” Reflecting 2 time(s)...\n"
     ]
    }
   ],
   "source": [
    "stats_analysis, stats_memory_id = statistician(\n",
    "    f\"Please analyze this product and UX research plan from a statistical perspective:\\n\\n\"\n",
    "    f\"Product Plan: {response}\\n\\n\"\n",
    "    f\"UX Research Plan: {ux_response}\",\n",
    "    num_reflections=2\n",
    ")\n",
    "print(remove_markdown(stats_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5c0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent. This is a superb synthesis of strategic product thinking and user-centric research planning. As your statistical and data science partner, my role is to help you reason through these trade-offs with statistical rigor, validating your approach and adding quantitative depth.\n",
      "\n",
      "First, the direct answer to your central question:\n",
      "\n",
      "From a statistical and causal inference perspective, the design challenge of creating a user experience that responsibly displays personal correlations is unequivocally the more difficult and higher-stakes challenge.\n",
      "\n",
      "The technical schema challenge is a problem of foresight and completeness; it is complex but can be solved with sufficient expertise and discipline. The design challenge is a problem of human psychology and cognitive bias; it is fundamentally ambiguous and directly threatens the integrity of your entire data asset. A \"perfect\" schema filled with data generated by users who are being systematically misled by the UI is worthless for causal inference. The user experience is upstream of data quality.\n",
      "\n",
      "Here is a full statistical analysis that integrates this perspective across your product and UX research plans.\n",
      "\n",
      "---\n",
      "\n",
      "1. Validation of Assumptions (Statistical Perspective)\n",
      "\n",
      "Your plan makes several key assumptions that your research must validate.\n",
      "\n",
      "*   Assumption: The \"Motivated Investigator\" provides high-quality, unbiased data.\n",
      "       Critique: Motivation reduces missing data* but does not eliminate measurement error. This self-reported data is subject to recall bias (imperfect memory of symptoms/triggers), reporting bias (seeing patterns one expects to see), and apophenia (the tendency to see patterns in random noise). A poorly designed UI can actively amplify these biases, polluting the data at the source.\n",
      "    *   Implication: Your \"causal-ready\" schema must be designed to model this uncertainty. The output of your \"User-Centered Ontology\" research should be a formal blueprint for the schema, specifying not just variables, but also crucial metadata to quantify data quality. Consider capturing:\n",
      "        *   data_entry_latency: The time delta between an event and when it was logged.\n",
      "        *   entry_method: Was it a quick-log widget or a detailed, reflective entry?\n",
      "        *   user_confidence_rating: An optional user-reported confidence score for a logged association.\n",
      "\n",
      "*   Assumption: The r/migraine beta cohort is sufficient for validating V1 KPIs.\n",
      "    *   Critique: This cohort introduces significant sampling bias. They are early adopters, more technically savvy, and more highly motivated than the general population of migraine sufferers. This limits the external validity (generalizability) of your findings.\n",
      "       Implication: The >60% adherence and >40 NPS are valid benchmarks for this specific persona*. Be prepared for these metrics to drop when you expand your go-to-market. Plan to re-baseline them with a more representative sample post-launch.\n",
      "\n",
      "*   Assumption: The NPS survey will be unbiased.\n",
      "    *   Critique: If you only survey users who complete the 30-day tracking period, you will create a massive survivorship bias, which will dramatically inflate your NPS. The most dissatisfied users are the ones who churned.\n",
      "       Implication: Your sampling frame for the NPS survey must* include users who churned. A simple one-question exit survey (\"What was the primary reason you stopped using the app?\") for churned users provides invaluable data to counterbalance the positive feedback from your engaged cohort.\n",
      "\n",
      "---\n",
      "\n",
      "2. Recommended Statistical Techniques\n",
      "\n",
      "Your UX research plan is sound. Here is how we can add quantitative rigor.\n",
      "\n",
      "*   For the Primary KPI (Adherence > 60%):\n",
      "       Recommended Test: Instead of a simple average, model adherence using Survival Analysis. A Kaplan-Meier curve will visualize the percentage of users remaining active over the 30-day period. This is far more insightful, as it will pinpoint exactly when* users tend to drop off (e.g., a steep drop after Day 7), allowing for targeted interventions.\n",
      "       Nuanced Metric (Actionable): I strongly recommend implementing a composite \"Data Richness Score\" per user. A user logging only one field is less valuable than a user logging four. A simple weighted formula makes this concrete: Score = (w_symptoms  n_symptoms) + (w_diet  n_diet_items) + (w_context  n_context_factors). This aligns your primary KPI directly with the strategic goal of building the Causal Data Asset.\n",
      "\n",
      "*   For the Secondary KPI (NPS > 40):\n",
      "    *   Recommended Technique: Use bootstrapping to calculate a 95% confidence interval around your NPS score. A single point estimate (e.g., 42) is misleading. A bootstrapped CI (e.g., [35, 52]) gives a much more honest assessment of the statistical uncertainty and the range of plausible true values.\n",
      "\n",
      "*   For Your UX Research Plan:\n",
      "    *   \"Comprehension Testing\": This is your primary defense against the design challenge. Codify the qualitative responses from \"What is this chart telling you?\" into distinct categories (e.g., \"Correct Cautious Interpretation,\" \"Incorrect Causal Leap,\" \"Confused\").\n",
      "    *   \"A/B Testing on Framing\": Use a Chi-Squared Test of Independence on the categorized comprehension data. This will allow you to state with statistical confidence whether Design A (e.g., using dotted lines and uncertainty language) is significantly better than Design B (e.g., using solid lines) at preventing users from making incorrect causal leaps. This provides rigorous, quantitative evidence to solve the design challenge.\n",
      "\n",
      "---\n",
      "\n",
      "3. Risk Warnings & Statistical Limitations\n",
      "\n",
      "*   The Multiple Comparisons Problem (P-Hacking):\n",
      "    *   Risk: Once you have data, the temptation to correlate everything with everything else is immense. If you run 100 statistical tests, you're likely to find about 5 \"significant\" results by pure chance (at Î±=0.05). This is the fastest way to lose scientific credibility.\n",
      "    *   Mitigation: Your Data Science lead must pre-specify a limited number of primary hypotheses. For any exploratory analysis, use statistical corrections for multiple comparisons, such as False Discovery Rate (FDR) control, which is more powerful than the overly conservative Bonferroni correction.\n",
      "\n",
      "*   Confounding Variables (The Core Causal Challenge):\n",
      "    *   Risk: Your plan correctly identifies this. A correlation between red wine and migraines might be confounded by stress (the true cause of both).\n",
      "       Mitigation: The V1 schema is your only* defense against confounding. You cannot control for a confounder you do not measure. Your \"Generative Interviews\" and \"User-Centered Ontology\" research are the most critical risk mitigation activities for the entire long-term causal vision. Their outputâ€”a structured knowledge graph of entities, attributes, and relationshipsâ€”must serve as the literal blueprint for the database schema.\n",
      "\n",
      "---\n",
      "\n",
      "4. Implementation Details (Sample Size & Data)\n",
      "\n",
      "*   Sample Size for V1 KPIs: Your target of 1,000 beta users is statistically robust.\n",
      "    *   Power Analysis: To confirm your adherence rate is significantly above a baseline (e.g., a null hypothesis of 50%), to detect a rate of 60% with 80% power (Î²=0.2) at a significance level of 5% (Î±=0.05), you need a minimum of n â‰ˆ 199 users who complete the trial.\n",
      "    *   Conclusion: Your target of 1,000 provides an excellent buffer for attrition and gives you high statistical power to not only validate the primary KPI but also to perform segmentation analysis (e.g., comparing adherence between different personas within your beta cohort).\n",
      "\n",
      "*   \"Causal-Ready\" Schema - Concrete Statistical Requirements:\n",
      "    1.  Precise Timestamps: Every entry must be timestamped, not just dated. The temporal ordering of events is the foundation of time-series causal analysis.\n",
      "    2.  Time-Varying vs. Static Covariates: The schema must clearly distinguish between data collected at onboarding (e.g., diagnosis, age) and time-varying data logged daily (e.g., stress, sleep).\n",
      "    3.  Data Provenance Metadata: As mentioned, log metadata about how data was entered (e.g., real-time vs. backfilled). This allows you to weight the quality and reliability of data points during analysis.\n",
      "\n",
      "---\n",
      "\n",
      "5. Causal Inference Considerations (Strengthening Your Roadmap)\n",
      "\n",
      "Your staged path is statistically sound. However, I propose a crucial addition: a parallel path to causality that can deliver value much sooner.\n",
      "\n",
      "*   V1 (\"Smart Diary\"): Goal is unchanged: assess the feasibility of collecting high-density, longitudinal, multivariate observational data. Success is measured by data liquidity and trust.\n",
      "\n",
      "*   V2 (\"Pattern Spotter\"): The Introduction of N-of-1 Trials\n",
      "    *   Observational Insights: For population-level correlations, you must use techniques like partial correlation to control for obvious confounders. Frame it carefully: \"After statistically adjusting for sleep quality, we still observe a weak correlation between X and Y in users like you.\"\n",
      "    *   A Parallel, More Powerful Path: Your \"N-of-1 Coach\" is the perfect vehicle to introduce structured, personalized Randomized Controlled Trials (RCTs). Instead of just observing, guide the user through an experiment:\n",
      "           Example:* \"Let's test if caffeine is a trigger for you. For the next 14 days, the app will randomly tell you each morning to either have your usual coffee or to avoid it. Log your symptoms as normal.\"\n",
      "           Benefit:* This creates experimental data, which, for that individual, is not subject to confounding. It allows you to deliver true, personalized causal insights much earlier, providing immense user value and generating a uniquely powerful dataset.\n",
      "\n",
      "*   V3/V4 (\"Causal Inference Engine\"):\n",
      "    *   Observational Methods: For your observational data, all advanced methods (Propensity Score Matching, etc.) rely on the untestable assumption of \"ignorability\" or \"no unmeasured confounders.\" This is an article of faith that you have measured everything important. This is why the V1 UX research is so critical.\n",
      "    *   Privacy-Preserving Analytics: To deliver population insights safely, you must plan for techniques like Differential Privacy. This involves adding carefully calibrated statistical noise to aggregate queries, making it mathematically impossible to determine if any single individual's data was included. This provides a formal privacy guarantee, which is a powerful way to build and maintain trust.\n",
      "    *   Synthesis: The ultimate vision (V4) becomes even more powerful. You can synthesize evidence from three sources: the scientific literature, your large-scale observational data, and the results from thousands of personalized N-of-1 trials run on your platform. This creates an evidence hierarchy that is truly defensible and unique.\n",
      "\n",
      "In summary, your plan is excellent. By prioritizing the design challenge, quantifying your UX research, adopting a parallel strategy of in-app N-of-1 trials, and planning for privacy-preserving technologies, you will build a platform that is not only statistically robust but also ethical, trustworthy, and positioned to deliver on its ambitious causal vision.\n"
     ]
    }
   ],
   "source": [
    "print(remove_markdown(stats_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1cc77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
