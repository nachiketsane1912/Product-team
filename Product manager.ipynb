{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71a6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18400c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from google import genai\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc620c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = genai.Client(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "model=\"gemini-2.5-pro\"\n",
    "tavily_client = TavilyClient(api_key=os.environ.get('TAVILY_API_KEY')) # Initialize Tavily client\n",
    "\n",
    "# Memory\n",
    "\n",
    "from memory_system import (\n",
    "    MemoryStore, MemoryRetriever, FeedbackHandler, \n",
    "    ContextBuilder, AgentMemoryIntegration\n",
    ")\n",
    "\n",
    "# Initialize memory system\n",
    "memory_store = MemoryStore(\"pm_agent_memory.db\")\n",
    "retriever = MemoryRetriever(memory_store)\n",
    "feedback_handler = FeedbackHandler(memory_store)\n",
    "context_builder = ContextBuilder(retriever)\n",
    "memory_integration = AgentMemoryIntegration(\n",
    "    memory_store, retriever, feedback_handler, context_builder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a37c11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agents\n",
    "class ProductManagerAgent:\n",
    "    \"\"\"Product Manager Agent with conversation memory and long-term learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.system_instruction = \"\"\"You are an experienced Product Manager with expertise in:\n",
    "        - Product strategy and roadmap planning\n",
    "        - Market research and competitive analysis\n",
    "        - User research and requirements gathering\n",
    "        - Feature prioritization and backlog management\n",
    "        - Stakeholder communication\n",
    "        - Metrics and KPI definition\n",
    "        - Go-to-market strategies\n",
    "        - Agile methodologies\n",
    "        - Providing feedback to your product specialist, engineering, design, data science, testing, and marketing teams\n",
    "\n",
    "        You provide practical, actionable advice and ask clarifying questions when needed.\n",
    "        When provided with search results or past memories, incorporate relevant information into your response and cite sources.\"\"\"\n",
    "    \n",
    "    def __call__(self, user_query):\n",
    "        \"\"\"\n",
    "        Make the agent callable like a function.\n",
    "        Returns (response, memory_id) tuple.\n",
    "        \"\"\"\n",
    "        # Get memory context\n",
    "        memory_context = self.memory_integration.pre_run_hook(user_query, \"pm_interaction\")\n",
    "\n",
    "        # Decide if search is needed\n",
    "        decision_prompt = f\"\"\"Analyze this query and determine if you need web search.\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\":\"\"\"\n",
    "\n",
    "        decision_response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=decision_prompt\n",
    "        )\n",
    "        \n",
    "        needs_search = \"YES\" in decision_response.text.strip().upper()\n",
    "        \n",
    "        # Perform search if needed\n",
    "        search_context = \"\"\n",
    "        if needs_search:\n",
    "            print(\"üîç Searching the web...\")\n",
    "            search_results = search_web(user_query)\n",
    "            if isinstance(search_results, list):\n",
    "                context = \"\\n\\nWeb Search Results:\\n\"\n",
    "                for i, result in enumerate(search_results, 1):\n",
    "                    search_context += f\"\\n{i}. {result.get('title', '')}\\n{result.get('content', '')}\\n\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "            for entry in self.conversation_history[-5:]:  # Keep last 5 exchanges\n",
    "                conversation_context += f\"User: {entry['user']}\\nAssistant: {entry['assistant']}\\n\\n\"\n",
    "        \n",
    "        # Combine all contexts\n",
    "        full_query = (\n",
    "            memory_context +\n",
    "            conversation_context + \n",
    "            f\"\\nCurrent query: {user_query}\" +\n",
    "            search_context\n",
    "        )\n",
    "        \n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=full_query,\n",
    "            config={'system_instruction': self.system_instruction}\n",
    "        )\n",
    "        \n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            'user': user_query,\n",
    "            'assistant': response.text\n",
    "        })\n",
    "        \n",
    "        # Log to long-term memory\n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            user_query,\n",
    "            response.text,\n",
    "            memory_type=\"pm_interaction\",\n",
    "            metadata={'search_used': needs_search}\n",
    "        )\n",
    "\n",
    "        return response.text, memory_id\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, \n",
    "                      feedback_content: str = \"\"):\n",
    "        \"\"\"Apply feedback to a specific interaction.\"\"\"\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"Conversation history cleared.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f93a3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def remove_markdown(response_text):\n",
    "    \"\"\"Remove markdown formatting and print clean text.\"\"\"\n",
    "    # Remove markdown formatting\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', response_text)  # Bold\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'`(.+?)`', r'\\1', text)  # Code\n",
    "    text = re.sub(r'#+\\s', '', text)  # Headers\n",
    "    return text\n",
    "def chat_with_agent(agent, agent_name=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Generic interactive chat interface for any agent.\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent instance (callable) to chat with\n",
    "        agent_name: Display name for the agent (default: \"Agent\")\n",
    "    \n",
    "    Type 'exit', 'quit', or 'bye' to end the conversation.\n",
    "    Type 'clear' to clear conversation history.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{agent_name} Chat\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Start chatting with the {agent_name}!\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the chat.\")\n",
    "    print(\"Type 'clear' to clear conversation history.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(f\"\\n{agent_name}: Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'clear':\n",
    "            if hasattr(agent, 'clear_history'):\n",
    "                agent.clear_history()\n",
    "            continue\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = agent(user_input)\n",
    "            print(f\"\\n{agent_name}: {remove_markdown(response)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\\n\")\n",
    "            print(\"Please try again or type 'exit' to quit.\\n\")\n",
    "def search_web(query):\n",
    "    \"\"\"Search the web using Tavily.\"\"\"\n",
    "    try:\n",
    "        response = tavily_client.search(query, max_results=5)\n",
    "        return response['results']\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c12a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent instances\n",
    "pm_agent = ProductManagerAgent(model=model, client=client, memory_integration=memory_integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35b69a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course. This is an excellent question that builds directly on our previous conversations about building a \"product team of agents\" and using prioritization frameworks like RICE.\n",
      "\n",
      "Our initial discussion laid out a strong strategic blueprint: a multi-agent system with specialized roles (Orchestrator, Researchers, Strategist, etc.), a phased workflow, and an incremental roadmap ([pm_interaction]).\n",
      "\n",
      "Now, let's move from that high-level strategy to a more detailed, operational plan. As a PM building this system, I'd focus on three critical areas to bring this concept to life:\n",
      "\n",
      "1.  Refining the Agent Workflow with a Prioritization Engine.\n",
      "2.  Designing the \"Human-in-the-Loop\" Interface as a Core Product Feature.\n",
      "3.  Defining a Granular Roadmap with Clear Success Metrics.\n",
      "\n",
      "---\n",
      "\n",
      "1. Refining the Agent Workflow with a Prioritization Engine\n",
      "\n",
      "In our last discussion, the Product Strategist Agent was tasked with creating a \"list of potential features.\" Let's make that process more robust by explicitly integrating the RICE framework we talked about ([pm_interaction]). This makes the system's output far more defensible and useful.\n",
      "\n",
      "Here‚Äôs the enhanced workflow, managed by the Orchestrator Agent:\n",
      "\n",
      "Step 1 & 2: Intake & Discovery (No Change)\n",
      "*   The system receives the user query (e.g., \"Build a mobile app for finding and booking local pickup sports games\").\n",
      "*   The Market and User Researcher Agents are dispatched to gather competitive analysis, user pain points, and create personas.\n",
      "\n",
      "Step 3: Synthesis & Feature Ideation (Enhancement)\n",
      "*   The Product Strategist Agent receives the research.\n",
      "*   Task: \"Based on the research, generate a list of 10-15 potential features or user stories that address the key pain points for our personas, the 'Casual Athlete' and 'Team Organizer'.\"\n",
      "*   Output: A raw, unprioritized list of features (e.g., Map-based search, Player skill-level filtering, In-app payments for game fees, Team chat, etc.).\n",
      "\n",
      "Step 4: Automated RICE Scoring (New & Critical Step)\n",
      "*   The Product Strategist Agent now evaluates each feature from Step 3 against the RICE criteria:\n",
      "       Reach: It estimates this based on the user research. Example Prompt:* \"Based on the personas, estimate how many of our target 10,000 monthly active users would use the 'Skill-Level Filtering' feature each month.\"\n",
      "       Impact: It quantifies the qualitative research. Example Prompt:* \"On a scale of 0.25 to 3, how significantly would 'In-App Payments' solve a stated pain point for the 'Team Organizer' persona?\"\n",
      "       Effort: This is the trickiest to automate. The agent would use a simplified model. Example Prompt:* \"Categorize the engineering effort for 'Team Chat' as Small (1), Medium (2), Large (4), or XL (8) person-weeks, based on typical feature complexity.\"\n",
      "\n",
      "Step 5: The Human-in-the-Loop (HITL) Confidence Check\n",
      "*   This is the most important step. The system presents the feature list and its calculated R, I, and E values to the human PM via a dedicated interface.\n",
      "*   The interface explicitly asks the human to provide the Confidence score (e.g., as a percentage from 50% to 100%) for each feature. As we discussed, this is where the PM's intuition and experience are injected ([pm_interaction]). The PM can also override any of the AI's other estimates.\n",
      "\n",
      "Step 6: Prioritized Backlog Generation\n",
      "*   With all four RICE variables, the system calculates the scores and presents a prioritized list of features.\n",
      "*   This prioritized list is then passed to the Solution Architect Agent to begin writing detailed user stories and acceptance criteria, and to the UX/UI Design Agent for wireframing.\n",
      "\n",
      "This enhanced workflow turns a simple \"feature list\" into a data-informed, defensible backlog that transparently combines AI-driven analysis with human expertise.\n",
      "\n",
      "2. Designing the \"Human-in-the-Loop\" Interface: The PM's Cockpit\n",
      "\n",
      "The HITL interface isn't just a pop-up; it's the core user experience of this product. It needs to be designed to facilitate trust, control, and collaboration.\n",
      "\n",
      "Here are the key features I'd specify for this interface:\n",
      "\n",
      "*   Stage-Gate Dashboards: The user shouldn't just get a final document. They should have dashboards to review and approve outputs at each major stage:\n",
      "    *   Research Review: See the personas and competitive analysis. Edit them directly in the UI.\n",
      "    *   Prioritization Workbench: The interface for Step 5 above. Show the feature list, the AI's R, I, E estimates, and a prominent slider/input box for the human PM's Confidence score.\n",
      "    *   Backlog & Design Review: View the generated user stories and wireframes side-by-side.\n",
      "*   Source Traceability: Every piece of data from the research agents must be linked to its source. If the AI claims \"the market for sports apps is growing at 15% YoY,\" there must be a clickable citation. This is critical for building trust.\n",
      "*   Editable & Regenerative Outputs: The user must be able to directly edit any text the AI generates. There should also be a \"Regenerate with this feedback\" button, allowing the user to correct the AI's course (e.g., \"This persona feels too generic; make them more specific to urban areas\"). This feedback is crucial for fine-tuning the underlying models.\n",
      "*   Version Control: The system should save \"snapshots\" of the project after each human approval, allowing the PM to see how the product definition has evolved and to roll back if needed.\n",
      "\n",
      "3. A More Granular Roadmap & Success Metrics\n",
      "\n",
      "Our previous roadmap ([pm_interaction]) was a great V1-V4 outline. Let's add more detail and define how we'll measure success for each phase.\n",
      "\n",
      "Phase 0: Foundational Model & Prompt Engineering\n",
      "*   Goal: Establish baseline quality for individual agent tasks.\n",
      "*   Activities: Test different LLMs. Build and rigorously test the core prompts for each agent in isolation.\n",
      "*   Metric: Output Quality Score (Rated 1-5 by an internal panel of senior PMs and engineers on clarity, accuracy, and completeness). We don't proceed until we achieve a consistent score of 4.0+.\n",
      "\n",
      "Phase 1 (V1): The \"Research Co-pilot\"\n",
      "*   Agents: Orchestrator, Market Researcher, User Researcher.\n",
      "*   Functionality: A user enters a product idea and gets a comprehensive, editable research report with cited sources.\n",
      "*   Primary KPI: Time to 'Approved Research Report'. We aim to reduce this from a manual process of days to under one hour.\n",
      "*   Secondary KPI: Human Edit Rate. What percentage of the AI-generated text needs to be modified by the user? Our goal is to drive this down over time.\n",
      "\n",
      "Phase 2 (V2): The \"Prioritization Engine\"\n",
      "*   Agents: Add the Product Strategist agent and the RICE prioritization workflow.\n",
      "*   Functionality: Builds on V1. After research is approved, the system generates a feature list and the RICE \"Workbench\" for human input.\n",
      "*   Primary KPI: User Confidence in Prioritization. Measured via a simple survey: \"How confident are you that this is the right priority for your MVP? (1-10)\".\n",
      "*   Secondary KPI: Feature Idea Diversity Score. Does the system generate non-obvious but valuable features?\n",
      "\n",
      "Phase 3 (V3): The \"Dev-Ready Backlog Generator\"\n",
      "*   Agents: Add the Solution Architect and UX/UI Design agents.\n",
      "*   Functionality: Generates user stories and wireframes for the top-priority features.\n",
      "*   Primary KPI: 'Dev-Ready' Acceptance Rate. What percentage of generated user stories and designs are accepted by an engineering lead without significant changes?\n",
      "*   Secondary KPI: Time to 'Dev-Ready Artifacts'. This becomes our North Star Metric for the entire system.\n",
      "\n",
      "By taking our initial strategic framework and layering in these operational details‚Äîa RICE-powered workflow, a feature-rich HITL interface, and a metrics-driven roadmap‚Äîwe move from a compelling idea to a practical and buildable product plan.\n",
      "\n",
      "Which part of this system‚Äîthe research agents, the prioritization engine, or the HITL interface‚Äîpresents the most interesting immediate challenge to you?\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "query = \"If you wanted to build a product team of agents, a team that can take a user query, do research, create user stories, designs, create different documents that engineers can use, how would you go about doint it?\"\n",
    "response, memory_id = pm_agent(query)\n",
    "print(remove_markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4f0bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course. The RICE framework is a classic and highly effective tool in a Product Manager's toolkit for prioritization. Let's break it down from a practical standpoint.\n",
      "\n",
      "What is the RICE Framework?\n",
      "\n",
      "At its core, RICE is a simple scoring system designed to help product teams prioritize features, initiatives, and ideas in a more objective, data-informed way. It forces you to think critically about why you're building something and helps you defend your roadmap decisions to stakeholders.\n",
      "\n",
      "The goal is to move beyond prioritizing based on the \"loudest voice in the room\" or pure gut instinct.\n",
      "\n",
      "The acronym RICE stands for:\n",
      "\n",
      "1.  Reach\n",
      "2.  Impact\n",
      "3.  Confidence\n",
      "4.  Effort\n",
      "\n",
      "You calculate a score for each feature using the formula:\n",
      "\n",
      "(Reach x Impact x Confidence) / Effort = RICE Score\n",
      "\n",
      "Let's dive into each component.\n",
      "\n",
      "---\n",
      "\n",
      "Breaking Down the RICE Components\n",
      "\n",
      "1. Reach\n",
      "*   The Question: How many people will this feature affect over a specific time period?\n",
      "*   How to Measure It: You need a real number. Avoid vague terms like \"many users.\" Define a time period (e.g., per month or per quarter) and estimate the number of users who will encounter the feature.\n",
      "*   Example: Let's imagine we're working on the \"pickup sports app\" we discussed in our previous conversation ([pm_interaction]).\n",
      "    *   Feature A: Redesign the Signup Flow. Reach = 500 new users per month (all new users experience this).\n",
      "    *   Feature B: Add Advanced Tools for Team Captains. Reach = 50 users per month (only 10% of our user base are captains).\n",
      "\n",
      "2. Impact\n",
      "*   The Question: How much will this feature impact an individual user when they encounter it?\n",
      "*   How to Measure It: This is more qualitative, so we use a simple scale to quantify it. A common scale is:\n",
      "    *   3 = Massive impact (a game-changer for the user's workflow)\n",
      "    *   2 = High impact\n",
      "    *   1 = Medium impact\n",
      "    *   0.5 = Low impact (a minor convenience)\n",
      "    *   0.25 = Minimal impact\n",
      "*   Example (continuing with the sports app):\n",
      "    *   Feature A (Signup Flow): A smoother flow is great, but it's a one-time action. Let's say the impact is Medium (1).\n",
      "    *   Feature B (Captain's Tools): This could save a captain hours of work organizing their team. The impact is Massive (3) for that specific user.\n",
      "\n",
      "3. Confidence\n",
      "*   The Question: How confident are you in your estimates for Reach and Impact?\n",
      "*   How to Measure It: This is expressed as a percentage. It‚Äôs a crucial gut check that tempers enthusiasm with realism.\n",
      "    *   100% = High confidence (I have quantitative data from user research or A/B tests to back this up).\n",
      "    *   80% = Medium confidence (I have qualitative research or strong market analysis, but no hard numbers yet).\n",
      "    *   50% = Low confidence (This is a \"big idea\" with little data to support it. It's more of a hunch).\n",
      "*   Example:\n",
      "    *   Feature A (Signup Flow): We have funnel data showing where users drop off. Our confidence in the impact of a redesign is High (100%).\n",
      "    *   Feature B (Captain's Tools): We've heard captains complain in interviews, but we haven't quantified how many would use this specific tool. Our confidence is Medium (80%).\n",
      "\n",
      "4. Effort\n",
      "*   The Question: How much total time will this require from your product, design, and engineering teams?\n",
      "*   How to Measure It: This is typically estimated in \"person-months\" or some other standardized unit (like story points or T-shirt sizes that you convert to a number). It's critical to include the full team's effort, not just development time.\n",
      "    *   Smallest projects = 0.5 person-months.\n",
      "*   Example (after consulting with the engineering lead):\n",
      "    *   Feature A (Signup Flow): This is a small, self-contained project. Effort = 1 person-month.\n",
      "    *   Feature B (Captain's Tools): This involves new database models and complex UI. Effort = 3 person-months.\n",
      "\n",
      "---\n",
      "\n",
      "Putting It All Together: A Worked Example\n",
      "\n",
      "Let's calculate the RICE scores for our two features:\n",
      "\n",
      "Feature A: Redesign the Signup Flow\n",
      "   (Reach: 500  Impact: 1 * Confidence: 100%) / Effort: 1\n",
      "   (500  1 * 1.0) / 1 = 500\n",
      "\n",
      "Feature B: Add Advanced Tools for Team Captains\n",
      "   (Reach: 50  Impact: 3 * Confidence: 80%) / Effort: 3\n",
      "   (50  3 * 0.8) / 3 = 120 / 3 = 40\n",
      "\n",
      "Conclusion: Based on the RICE score, redesigning the signup flow (Score: 500) is the higher-priority item, even though the tools for captains feel more impactful on an individual level. It delivers more overall value for the effort invested.\n",
      "\n",
      "Pros and Cons (My PM Advice)\n",
      "\n",
      "When to Use RICE:\n",
      "*   When you have many competing ideas and need a structured way to compare them.\n",
      "*   When you need to explain your roadmap to leadership in a data-driven way.\n",
      "*   When you want to reduce personal bias in decision-making.\n",
      "\n",
      "Potential Pitfalls:\n",
      "*   \"Pseudo-science\" Risk: The numbers are only as good as your estimates. If your inputs are garbage, your score will be too. Always be ready to explain the \"why\" behind your numbers.\n",
      "   Doesn't Account for Strategy: RICE won't tell you if a feature is a \"strategic bet\" or \"table stakes\" (something you must* have to compete). A low-scoring feature might still be necessary for strategic reasons. You have to layer your product sense on top of the score.\n",
      "*   Dependencies: RICE doesn't inherently manage dependencies. You can't build Feature Y if it depends on Feature X, even if Y has a higher score.\n",
      "\n",
      "Connecting This to Our \"Product Team of Agents\"\n",
      "\n",
      "In the system we discussed previously ([pm_interaction]), the Product Strategist Agent would be the perfect user of the RICE framework. After the Market and User Researcher agents provide their findings, the Strategist agent could:\n",
      "1.  Generate a list of potential features.\n",
      "2.  Estimate the Reach and Impact for each feature based on the research data.\n",
      "3.  Estimate the Effort by using a model trained on past project data.\n",
      "4.  Crucially, it would then use the Human-in-the-Loop (HITL) interface to ask the human PM for the Confidence score. This keeps the PM's expertise and intuition in the driver's seat.\n",
      "5.  Finally, it would present a prioritized backlog based on the calculated RICE scores, ready for review.\n",
      "\n",
      "Do you have any questions about how you might apply this to a specific project?\n",
      "\n",
      "Memory ID: 2\n",
      "Feedback 'positive' applied to memory 2\n"
     ]
    }
   ],
   "source": [
    "# Test with memory\n",
    "response, memory_id = pm_agent(\"What is the RICE framework?\")\n",
    "print(remove_markdown(response))\n",
    "print(f\"\\nMemory ID: {memory_id}\")\n",
    "\n",
    "# Apply feedback\n",
    "pm_agent.apply_feedback(memory_id, 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe3349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
