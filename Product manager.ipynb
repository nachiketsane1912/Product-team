{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71a6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18400c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from google import genai\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any, Dict, Optional\n",
    "from memory_system import (\n",
    "    MemoryStore, MemoryRetriever, FeedbackHandler, \n",
    "    ContextBuilder, AgentMemoryIntegration\n",
    ")\n",
    "from reflection_system import ReflectionWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc620c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = genai.Client(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "model=\"gemini-2.5-pro\"\n",
    "tavily_client = TavilyClient(api_key=os.environ.get('TAVILY_API_KEY')) # Initialize Tavily client\n",
    "\n",
    "# Memory\n",
    "\n",
    "# Initialize memory system\n",
    "memory_store = MemoryStore(\"pm_agent_memory.db\")\n",
    "retriever = MemoryRetriever(memory_store)\n",
    "feedback_handler = FeedbackHandler(memory_store)\n",
    "context_builder = ContextBuilder(retriever)\n",
    "memory_integration = AgentMemoryIntegration(\n",
    "    memory_store, retriever, feedback_handler, context_builder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a37c11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agents\n",
    "class ProductManagerAgent:\n",
    "    \"\"\"Product Manager Agent with conversation memory and long-term learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, client, memory_integration: AgentMemoryIntegration, num_reflections: int = 0):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.memory_integration = memory_integration\n",
    "        self.conversation_history = []\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow = ReflectionWorkflow(client, model, num_reflections)\n",
    "        self.system_instruction = \"\"\"You are an experienced Product Manager with expertise in:\n",
    "        - Product strategy and roadmap planning\n",
    "        - Market research and competitive analysis\n",
    "        - User research and requirements gathering\n",
    "        - Feature prioritization and backlog management\n",
    "        - Stakeholder communication\n",
    "        - Metrics and KPI definition\n",
    "        - Go-to-market strategies\n",
    "        - Agile methodologies\n",
    "        - Providing feedback to your product specialist, engineering, design, data science, testing, and marketing teams\n",
    "\n",
    "        You provide practical, actionable advice and ask clarifying questions when needed.\n",
    "        When provided with search results or past memories, incorporate relevant information into your response and cite sources.\"\"\"\n",
    "    \n",
    "    def __call__(self, user_query: str, num_reflections: Optional[int] = None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Make the agent callable like a function.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's query\n",
    "            num_reflections: Number of reflections for this specific query (overrides default)\n",
    "            verbose: If True, returns reflection data\n",
    "            \n",
    "        Returns:\n",
    "            (response, memory_id) tuple or (response, memory_id, reflection_data) if verbose\n",
    "        \"\"\"\n",
    "        # Use query-specific reflections or fall back to instance default\n",
    "        reflections_to_use = num_reflections if num_reflections is not None else self.num_reflections\n",
    "        \n",
    "        # Get memory context\n",
    "        memory_context = self.memory_integration.pre_run_hook(user_query, \"pm_interaction\")\n",
    "\n",
    "        # Decide if search is needed\n",
    "        decision_prompt = f\"\"\"Analyze this query and determine if you need web search.\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\":\"\"\"\n",
    "\n",
    "        decision_response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=decision_prompt\n",
    "        )\n",
    "        \n",
    "        needs_search = \"YES\" in decision_response.text.strip().upper()\n",
    "        \n",
    "        # Perform search if needed\n",
    "        search_context = \"\"\n",
    "        if needs_search:\n",
    "            print(\"ðŸ” Searching the web...\")\n",
    "            search_results = search_web(user_query)\n",
    "            if isinstance(search_results, list):\n",
    "                search_context = \"\\n\\nWeb Search Results:\\n\"\n",
    "                for i, result in enumerate(search_results, 1):\n",
    "                    search_context += f\"\\n{i}. {result.get('title', '')}\\n{result.get('content', '')}\\n\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation_context = \"\"\n",
    "        if self.conversation_history:\n",
    "            conversation_context = \"\\n\\nPrevious conversation:\\n\"\n",
    "            for entry in self.conversation_history[-5:]:  # Keep last 5 exchanges\n",
    "                conversation_context += f\"User: {entry['user']}\\nAssistant: {entry['assistant']}\\n\\n\"\n",
    "\n",
    "        # Define response generation function for reflection workflow\n",
    "        def generate_response(query: str) -> str:\n",
    "            full_query = (\n",
    "                memory_context +\n",
    "                conversation_context + \n",
    "                f\"\\n{query}\" +\n",
    "                search_context\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=full_query,\n",
    "                config={'system_instruction': self.system_instruction}\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        # Execute with or without reflection\n",
    "        if reflections_to_use > 0:\n",
    "            print(f\"ðŸ¤” Reflecting {reflections_to_use} time(s)...\")\n",
    "            # Temporarily update reflection workflow\n",
    "            self.reflection_workflow.num_reflections = reflections_to_use\n",
    "            result = self.reflection_workflow.execute(\n",
    "                initial_query=user_query,\n",
    "                generate_response_fn=generate_response,\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "            final_response = result['final_response']\n",
    "            reflection_data = result\n",
    "        else:\n",
    "            final_response = generate_response(user_query)\n",
    "            reflection_data = None\n",
    "\n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            'user': user_query,\n",
    "            'assistant': final_response\n",
    "        })\n",
    "        \n",
    "        # Log to long-term memory\n",
    "        memory_id = self.memory_integration.post_run_hook(\n",
    "            user_query,\n",
    "            final_response,\n",
    "            memory_type=\"pm_interaction\",\n",
    "            metadata={\n",
    "                'search_used': needs_search, \n",
    "                'num_reflections': reflections_to_use, \n",
    "                'reflection_used': reflections_to_use > 0\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if verbose and reflection_data:\n",
    "            return final_response, memory_id, reflection_data\n",
    "        \n",
    "        return final_response, memory_id\n",
    "    \n",
    "    def set_reflections(self, num_reflections: int):\n",
    "        \"\"\"Update the default number of reflections.\"\"\"\n",
    "        self.num_reflections = num_reflections\n",
    "        self.reflection_workflow.num_reflections = num_reflections\n",
    "        print(f\"Default reflection count updated to: {num_reflections}\")\n",
    "    \n",
    "    def apply_feedback(self, memory_id: int, feedback_type: str, \n",
    "                      feedback_content: str = \"\"):\n",
    "        \"\"\"Apply feedback to a specific interaction.\"\"\"\n",
    "        self.memory_integration.apply_feedback(memory_id, feedback_type, feedback_content)\n",
    "        print(f\"Feedback '{feedback_type}' applied to memory {memory_id}\")\n",
    "        \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.reflection_workflow.clear_history()\n",
    "        print(\"Conversation history cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f93a3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def remove_markdown(response_text):\n",
    "    \"\"\"Remove markdown formatting and print clean text.\"\"\"\n",
    "    # Remove markdown formatting\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', response_text)  # Bold\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'`(.+?)`', r'\\1', text)  # Code\n",
    "    text = re.sub(r'#+\\s', '', text)  # Headers\n",
    "    return text\n",
    "def chat_with_agent(agent, agent_name=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Generic interactive chat interface for any agent.\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent instance (callable) to chat with\n",
    "        agent_name: Display name for the agent (default: \"Agent\")\n",
    "    \n",
    "    Type 'exit', 'quit', or 'bye' to end the conversation.\n",
    "    Type 'clear' to clear conversation history.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{agent_name} Chat\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Start chatting with the {agent_name}!\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the chat.\")\n",
    "    print(\"Type 'clear' to clear conversation history.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(f\"\\n{agent_name}: Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'clear':\n",
    "            if hasattr(agent, 'clear_history'):\n",
    "                agent.clear_history()\n",
    "            continue\n",
    "            \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = agent(user_input)\n",
    "            print(f\"\\n{agent_name}: {remove_markdown(response)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\\n\")\n",
    "            print(\"Please try again or type 'exit' to quit.\\n\")\n",
    "def search_web(query):\n",
    "    \"\"\"Search the web using Tavily.\"\"\"\n",
    "    try:\n",
    "        response = tavily_client.search(query, max_results=5)\n",
    "        return response['results']\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c12a4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent instances\n",
    "pm_agent = ProductManagerAgent(model=model, client=client, memory_integration=memory_integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b69a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"If you wanted to build an app that lets users track their daily routine and interventions, and use counterfactuals based on other users and counterfactuals using quality research to provide them with causal impact of their interventions (for e.g., you drink black coffee every morning leads to higher energy levels) or also suggests changes to their life based on causal analysis, how would you go about doing it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching the web...\n",
      "ðŸ¤” Reflecting 1 time(s)...\n",
      "Of course. You've provided my previous strategic breakdown and a very insightful critique of it. This is exactly how a product strategy evolvesâ€”from a high-level vision to a battle-tested, tactical plan that anticipates real-world challenges.\n",
      "\n",
      "My initial response was a solid V1 plan. Your feedback correctly identifies the critical elements that would turn it from a good plan into a great, shippable one. Let's integrate that feedback and create a more robust, actionable V1 execution plan.\n",
      "\n",
      "Here is the improved, more detailed approach to move V1: The \"Smart Diary\" from strategy to execution.\n",
      "\n",
      "From Strategy to Execution: A More Robust Plan for V1 - The \"Smart Diary\"\n",
      "\n",
      "Our V1 goal remains the same, but our approach will be more rigorous: \"Can we get a specific user group to track their subjective and objective data consistently?\" Failure here invalidates the entire product concept, so we must de-risk it ruthlessly.\n",
      "\n",
      "Here are the concrete, updated steps.\n",
      "\n",
      "1. Author the V1 Product Brief (The Comprehensive One-Pager)\n",
      "\n",
      "This document is our single source of truth for the entire team. It needs to be sharp, focused, and anticipate key challenges.\n",
      "\n",
      "*   Problem Statement: \"Individuals managing chronic, fluctuating conditions like migraines or IBS lack a simple, unified tool to identify potential triggers. They are forced to use a combination of disconnected apps and manual notes, making it nearly impossible to discover meaningful patterns and feel in control of their health.\"\n",
      "\n",
      "*   V1 Hypothesis (Sharpened): \"We believe that by focusing on a single user cohort (migraine sufferers) and providing a sub-30-second daily logging flow, we can achieve >60% daily tracking adherence over a 30-day period. We believe this will succeed because existing tools are generic, high-friction, and not designed for the specific variables relevant to this group.\"\n",
      "\n",
      "*   Scope - What's In for V1:\n",
      "    *   A frictionless daily logging flow (text tags, sliders for pain levels, etc.).\n",
      "    *   Apple Health / Google Fit integration for objective data (sleep, steps, heart rate).\n",
      "    *   Simple correlation visualizations (e.g., sleep duration vs. reported pain level).\n",
      "    *   A secure, encrypted data backend.\n",
      "\n",
      "*   Scope - What's Out for V1 (Disciplined Exclusion):\n",
      "    *   No causal claims, AI insights, N-of-1 trials, or \"Digital Twin\" features. We are a \"smart diary,\" not yet a \"causal engine.\"\n",
      "\n",
      "*   Foundational Requirements (Non-Negotiables):\n",
      "    *   Data Privacy & Security: This is a Day Zero requirement. We will operate on the principle of least privilege. All user data will be encrypted at rest and in transit. We will design a clear, simple user consent flow that is GDPR and HIPAA-compliant from the start.\n",
      "    *   Analytics & Instrumentation: We cannot measure our KPIs if we are flying blind. V1 must include an analytics SDK (e.g., Mixpanel, Amplitude) to track key events: app_open, log_submitted, visualization_viewed, streak_achieved.\n",
      "\n",
      "*   V1 Go-to-Market (GTM) Plan:\n",
      "    *   Beachhead Market: We will not launch broadly. Our initial focus will be hyper-targeted on the r/migraine subreddit and partnerships with 1-2 migraine-focused health bloggers.\n",
      "    *   Launch Strategy: We will launch via a private TestFlight/closed Android beta with a cap of 1,000 users. This allows us to gather direct feedback, manage expectations, and ensure stability before a wider release.\n",
      "\n",
      "*   Key Metrics:\n",
      "    *   Primary KPI: Tracking Consistency (Adherence Rate). Target: >60% of days logged in the first 30 days.\n",
      "    *   Secondary KPIs: Day 7 Retention, User-reported satisfaction (NPS or similar), successful HealthKit/Google Fit connections.\n",
      "\n",
      "2. UX De-risking & Design (Validate Before Building)\n",
      "\n",
      "Before we storyboard a single final screen, we must validate our core assumption about what \"frictionless\" means to our users.\n",
      "\n",
      "   Prototype & Test: We will create a simple, non-functional prototype of the logging flow in Figma. I will then personally conduct user research sessions with 5-7 individuals who actively manage migraines. The goal is to answer: \"Can they complete their daily log in under 30 seconds? What are the friction points?\" We will iterate on the prototype based on this feedback before* the kick-off.\n",
      "*   The \"Magic Moment\" for V1 (Refined): While a user might see a cool correlation, that's not a guaranteed experience. Our reliable \"magic moment\" will be the feeling of accomplishment and control. We will deliver this by:\n",
      "    *   Celebrating Streaks: \"You've tracked for 7 days in a row! You're building a powerful dataset about your health.\"\n",
      "    *   The Weekly Summary: A beautifully designed, shareable summary of what the user has tracked. This provides tangible value and a sense of progress, regardless of the correlations found.\n",
      "\n",
      "3. Hold a V1 Kick-Off with the Cross-Functional Team\n",
      "\n",
      "Armed with the comprehensive brief and prototype feedback, the kick-off will be much more effective.\n",
      "\n",
      "*   To the Engineering Lead: \"Our two biggest technical challenges are security and data structure. We must build a HIPAA-compliant backend from day one. Secondly, our analytics instrumentation for tracking adherence is just as important as the user-facing features. Let's ensure that's in the first sprint.\"\n",
      "*   To the Design Lead: \"User research showed that speed and clarity are key. Let's focus on that sub-30-second flow. Crucially, we also need to design a crystal-clear user consent and privacy flow. This is our first and most important chance to earn user trust.\"\n",
      "*   To the Data Science Lead: \"Your guidance on data structure for V1 is critical. Please help us define the schema and metadata needed now, so that when we build the V2 causality engine, we have a clean, high-quality dataset to work with.\"\n",
      "\n",
      "4. Prioritize the V1 Feature Backlog Using RICE\n",
      "\n",
      "This data-informed exercise helps us align on what to build first. The RICE framework ([pm_interaction]) remains the right tool.\n",
      "\n",
      "| Feature | Reach (Users/mo) | Impact (0.25-3) | Confidence (%) | Effort (person-mo) | RICE Score |\n",
      "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
      "| Frictionless Text/Tag Input | 1000 | 3 (Massive) | 80% | 1.5 | 1600 |\n",
      "| Basic Correlation Graph | 800 | 2 (High) | 80% | 1 | 1280 |\n",
      "| Apple HealthKit Sync | 500 (iOS users) | 2 (High) | 100% | 1 | 1000 |\n",
      "| Celebrate Tracking Streaks | 900 | 1 (Medium) | 90% | 0.5 | 1620 |\n",
      "\n",
      "(Note the changes here: After some thought, 'Celebrate Tracking Streaks' scores higher because it has high reach, medium impact, and is very low effortâ€”a classic \"quick win\" that directly supports our primary KPI. The confidence for our core 'Frictionless Input' feature is now a more realistic 80%, as it's an assumption we're validating with prototypes.)\n",
      "\n",
      "This more robust plan directly addresses the key risks of data privacy, user acquisition, and measurement from the outset. It ensures we're not just building features, but building a foundation of trust and data that will allow our future, more ambitious versions to succeed.\n",
      "\n",
      "Given this detailed V1 plan, what do you see as the single biggest execution risk we now face: the technical challenge of a secure backend, the marketing challenge of recruiting our first 1,000 beta users, or the design challenge of truly nailing the sub-30-second input flow?\n"
     ]
    }
   ],
   "source": [
    "response, memory_id = pm_agent(query, num_reflections=0)\n",
    "print(remove_markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cf222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
